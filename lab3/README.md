# Lab 3 : Policy Serch
Each class contains a next_move method that gives the move to play in format tuple(row, n_elements). You can play with all of these strategies using the `play` function contained in the *util.py* that takes two strategy as Callable and a nim game. 
This code has been written by Serra Matteo s303513 and Magnaldi Matteo s296852 for the course of Computational Intelligence ay 2022-2023. The goal of this code is to play the nim game with an expert system or an evaluated rule strategy. To do that we wrote 3 methods:
- `expert_system` : this function use an informed system to make the next move, all the choices done in this function are made with the nim sum approach, if we specify a `k` as limit of elements to take this function will resolve the *subtractive game*. 
- the second approach use an evolutionary algorithm with tournament to tune the porbabilities of some simple policies. 
- `minmax` : all the code is contained in minmax.py, the class `MinMaxAgent` contains a contructor that instanciates the cache used by the `minmax` method. To play with this approach you have to instanciate an object of this class and then call iteratively (until the end of the game) the `next_move` method that take as parameter the current nim game. This method first look for the state in its cache, if it doesn't find it, it will call the `minmax` method. 
- `RL agent`: we wrote 2 versions of this approach: one taken from the professor and the other from an assignment of the Oxford university, we spent much efforts in the first one, infact it performs much better than the latter. Nevertheless the 'Oxford' one is sometime able to win against the expert system. The former version is avaiable in the *agent.py* file and has the other agent it has a method that gives the next move given the current state (`choose_action`). **NB** You have to train all the RL agents before playing with them. This agent has been trained with all the above agents (excepts the GAs) starting from the weakers and arriving to the strongest. This agent has the peculiarity of rewarding the agent when he arrives in a secure state (nim sum equals to zero). The other agent follow a similar idea but it rewards only the winner/loser state (+1/-1) and it has been trained only against itself and against the expert system, this method follow the qualitative learning and examines also the one distance future steps with a discounting factor of 1. 
- The evolutionary approach has been developed using a tournament approach (a policy play against an other policy) and the fitness function is the number of wins. In this approach if a policy wins against the other it increases the probability of using that policy in the 'official games'.  
## Results
The expert system is the optimale player so it can reach a 100% win ration against a random system and a 50% against another expert system. This means that every time he makes the second move it will win.
The evolutionary algorithm reached quite good results with a 70% win ratio against a random strategy opponent, but it can't win against the expert system.
The minmax method reach nearly optimal results, reaching a close to 50% win ratio against the expert system. 
The first implementation of the RL agent is quite good and can win against the Expert System indicatevly 10% of the times.

